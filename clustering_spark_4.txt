import logging
import warnings
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.sql.functions import col, mean
import matplotlib.pyplot as plt
import time

# Suppress warnings
warnings.filterwarnings('ignore')

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("KMeans_Clustering") \
    .config("spark.ui.showConsoleProgress", "false") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")  # Hide warnings

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def log_step(step_description):
    """Logs and prints the progress."""
    logging.info(step_description)
    print(f"âœ” {step_description}")

# Start time tracking
start_time = time.time()

# ðŸš€ Step 1: Load Raw Data
log_step("Loading raw data from Parquet...")
df_spark = spark.read.parquet("your_raw_data.parquet")

# Display schema
df_spark.printSchema()

# ðŸš€ Step 2: Identify Categorical and Numerical Columns
categorical_cols = [col for col, dtype in df_spark.dtypes if dtype == 'string']
numerical_cols = [col for col, dtype in df_spark.dtypes if dtype in ('int', 'double', 'float')]

log_step(f"Identified Categorical Columns: {categorical_cols}")
log_step(f"Identified Numerical Columns: {numerical_cols}")

# ðŸš€ Step 3: Fill Missing Values
log_step("Handling missing values in numerical and categorical columns...")

# Fill numerical missing values with column mean
if numerical_cols:
    impute_values = df_spark.select(*[mean(col(c)).alias(c) for c in numerical_cols]).collect()[0]
    df_spark = df_spark.fillna({c: impute_values[c] for c in numerical_cols})

# Fill categorical missing values with the most frequent value (mode)
for c in categorical_cols:
    mode_value = df_spark.groupBy(c).count().orderBy("count", ascending=False).first()[0]
    df_spark = df_spark.fillna({c: mode_value})

log_step("Missing values handled successfully.")

# ðŸš€ Step 4: Encode Categorical Features
if categorical_cols:
    log_step("Encoding categorical features using StringIndexer...")
    for col in categorical_cols:
        indexer = StringIndexer(inputCol=col, outputCol=col+"_index").fit(df_spark)
        df_spark = indexer.transform(df_spark)
    categorical_cols = [col+"_index" for col in categorical_cols]  # Update list with new indexed columns

# ðŸš€ Step 5: Scale Only Numerical Features
log_step("Scaling only numerical features using StandardScaler...")

# Assemble only numerical features for scaling
num_assembler = VectorAssembler(inputCols=numerical_cols, outputCol="numerical_features_unscaled")
df_spark = num_assembler.transform(df_spark)

# Apply StandardScaler **only on numerical features**
scaler = StandardScaler(inputCol="numerical_features_unscaled", outputCol="numerical_features_scaled", withStd=True, withMean=True)
df_spark = scaler.fit(df_spark).transform(df_spark)

# ðŸš€ Step 6: Assemble Encoded Categorical + Scaled Numerical Features
log_step("Assembling categorical and scaled numerical features...")

final_feature_cols = ["numerical_features_scaled"] + categorical_cols  # Include scaled numerical and encoded categorical

assembler = VectorAssembler(inputCols=final_feature_cols, outputCol="features")
df_spark = assembler.transform(df_spark)

# ðŸš€ Step 7: Verify Features Column
df_spark.printSchema()
df_spark.select("features").show(5)

# ðŸš€ Step 8: Determine Optimal k (Elbow & Silhouette)
log_step("Determining the optimal number of clusters (k)...")

costs = []
silhouette_scores = []
K_range = range(2, 10)
best_k = 2
best_score = -1

for k in K_range:
    log_step(f"Training K-Means for k={k}...")
    
    kmeans = KMeans(featuresCol="features", k=k, seed=42)
    model = kmeans.fit(df_spark)

    # Elbow Method (Inertia)
    inertia = model.summary.trainingCost
    costs.append(inertia)

    # Silhouette Score
    predictions = model.transform(df_spark)
    evaluator = ClusteringEvaluator(featuresCol="features", metricName="silhouette")
    silhouette_score = evaluator.evaluate(predictions)
    silhouette_scores.append(silhouette_score)

    log_step(f"  â†’ Inertia: {inertia:.2f}, Silhouette Score: {silhouette_score:.4f}")

    if silhouette_score > best_score:
        best_score = silhouette_score
        best_k = k

log_step(f"Optimal k based on Silhouette Score: {best_k}")

# ðŸš€ Step 9: Plot Elbow Method
log_step("Plotting the Elbow Method curve...")
plt.figure(figsize=(8, 5))
plt.plot(K_range, costs, marker='o', linestyle='--')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Cost)')
plt.title('Elbow Method for Optimal k')
plt.show()

# ðŸš€ Step 10: Train Final K-Means Model
log_step(f"Training final K-Means model with k={best_k}...")
kmeans = KMeans(featuresCol="features", k=best_k, seed=42)
model = kmeans.fit(df_spark)

# ðŸš€ Step 11: Assign Clusters
log_step("Assigning clusters to dataset...")
df_spark = model.transform(df_spark)

# ðŸš€ Step 12: Save Clustered Data
log_step("Saving clustered data to Parquet...")
df_spark.write.mode("overwrite").parquet("clustered_data.parquet")

# ðŸš€ Step 13: Display Results
log_step("Fetching clustered data for visualization...")
df_pandas = df_spark.select("Cluster").toPandas()

import ace_tools as tools
tools.display_dataframe_to_user(name="Clustered Data", dataframe=df_pandas)

# ðŸš€ Final Step: Process Complete!
end_time = time.time()
log_step(f"Clustering process completed in {round(end_time - start_time, 2)} seconds! ðŸŽ‰")

