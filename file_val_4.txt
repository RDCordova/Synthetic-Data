# -*- coding: utf-8 -*-
"""
DataFrame Validator against JSON DDL
- Reads DDL from a JSON file
- Validates presence, nullability, types, ranges, lengths, categories
- Robust datetime parsing (ISO-8601, tz offsets); supports date-only comparison
- Returns row-level errors and a detailed summary (with reasons)
"""

import json
from typing import Dict, List
import pandas as pd
import numpy as np

# -------------------- Global options --------------------
# Default comparison mode for DATE/DATETIME/TIMESTAMP columns:
#   "datetime" -> compare full timestamp (with time)
#   "date"     -> compare by calendar date only (ignoring time)
_DATE_COMPARE_MODE = "datetime"  # change to "date" if you want date-only checks globally

# Accepted plain date formats for legacy strings without timezone (still supported)
DATE_FORMATS = ["%Y-%m-%d", "%Y%m%d", "%Y/%m/%d", "%m/%d/%Y"]

# -------------------- I/O --------------------

def load_ddl(path: str) -> dict:
    """Load the DDL JSON file and return it as a Python dict."""
    with open(path, "r") as f:
        data = f.read()          # read as string
    return json.loads(data)      # parse into dict

# -------------------- Date/Time helpers --------------------

def parse_datetime_flex(series: pd.Series) -> pd.Series:
    """
    Robustly parse datetimes:
      - Accepts ISO8601 strings, fractional seconds, and timezone offsets (e.g., ...-05:00)
      - Works with existing datetime-typed Series (naive or tz-aware)
    Returns UTC-aware pandas Timestamps (datetime64[ns, UTC]) or NaT.
    """
    s = series
    # If already datetime64
    if np.issubdtype(s.dtype, np.datetime64):
        try:
            # If tz-aware, convert to UTC; if naive, localize as UTC (no shift)
            if getattr(s.dt, "tz", None) is not None:
                return s.dt.tz_convert("UTC")
            else:
                return s.dt.tz_localize("UTC")
        except Exception:
            # Fall back to generic parsing if something odd happens
            pass

    # Generic robust parse from strings/objects
    # utc=True gives tz-aware results; handles offsets like -05:00
    parsed = pd.to_datetime(s, errors="coerce", utc=True, infer_datetime_format=True)
    return parsed

def coerce_to_date(dt_utc: pd.Series) -> pd.Series:
    """
    Convert UTC-aware datetimes to date-only (drops time).
    Keeps NaT as-is.
    """
    # Normalize to midnight UTC, then grab date
    return dt_utc.dt.tz_convert("UTC").dt.normalize().dt.date

def _try_parse_dates(series: pd.Series, formats: List[str] = DATE_FORMATS) -> pd.Series:
    """
    Legacy helper for plain date strings without tz. Preserves NaN.
    Retained for parsing min/max date bounds that might be simple dates.
    """
    s = series.astype("string")
    out = pd.Series(pd.NaT, index=s.index, dtype="datetime64[ns]")
    remaining = s.notna()

    for fmt in formats:
        if not remaining.any():
            break
        parsed = pd.to_datetime(s[remaining], format=fmt, errors="coerce")
        good = parsed.notna()
        good_idx = parsed.index[good]
        out.loc[good_idx] = parsed.loc[good_idx]
        remaining.loc[good_idx] = False

    return out

# -------------------- DDL helpers --------------------

def _ddl_columns_by_name(ddl: dict, normalize: bool = False) -> Dict[str, dict]:
    """Flatten DDL to {column_name: column_def}. Optionally strip names."""
    by_name = {}
    for block in ddl.get("ddl", []):
        for c in block.get("columns", []):
            name = c["name"]
            if normalize and isinstance(name, str):
                name = name.strip()
            by_name[name] = c
    return by_name

# -------------------- Core Validator --------------------

def validate_df_against_ddl(
    df: pd.DataFrame,
    ddl: dict,
    *,
    fail_on_unexpected: bool = False,
    normalize_column_names: bool = False
):
    """
    Validate df against ddl.

    Returns:
      errors_df: DataFrame[row_index, column, issue, value]
      summary: {
        valid: bool,
        reasons: List[str],
        missing_required_columns: List[str],
        unexpected_columns: List[str],
        columns: Dict[col, {"errors": int, "nulls": int|None}],
        overlap_columns: List[str],
        ddl_only_columns: List[str],
        df_only_columns: List[str],
      }
    """
    # Optional normalization of column names
    if normalize_column_names:
        df = df.rename(columns=lambda c: c.strip() if isinstance(c, str) else c)

    by_name = _ddl_columns_by_name(ddl, normalize=normalize_column_names)
    ddl_cols = set(by_name.keys())
    df_cols = set(df.columns)

    required = {c for c, d in by_name.items()
                if not (d.get("type", {}) or {}).get("nullable", True)}
    missing_required = sorted(list(required - df_cols))
    unexpected = sorted(list(df_cols - ddl_cols))

    col_summary = {
        c: {"errors": 0, "nulls": (int(df[c].isna().sum()) if c in df_cols else None)}
        for c in ddl_cols
    }

    errors = []

    # Validate only overlapping columns
    for col in sorted(ddl_cols & df_cols):
        spec = by_name[col]
        typ = (spec.get("type", {}) or {}).get("datatype", "").upper()
        nullable = (spec.get("type", {}) or {}).get("nullable", True)
        declared_max_len = (spec.get("type", {}) or {}).get("maxLength", None)
        meta = spec.get("metadata", {}) or {}

        s = df[col]
        null_mask = s.isna() | (s.astype("string").str.strip() == "")

        # Nullability
        if not nullable and null_mask.any():
            n_bad = int(null_mask.sum())
            col_summary[col]["errors"] += n_bad
            for i in s.index[null_mask]:
                errors.append({"row_index": int(i), "column": col,
                               "issue": "Null/empty but not nullable", "value": None})

        nn = s[~null_mask]

        # INT
        if typ in ("INT", "INTEGER"):
            coerced = pd.to_numeric(nn, errors="coerce", downcast="integer")
            bad_type = coerced.isna()
            if bad_type.any():
                n_bad = int(bad_type.sum()); col_summary[col]["errors"] += n_bad
                for i, v in nn[bad_type].items():
                    errors.append({"row_index": int(i), "column": col, "issue": "Expected INT", "value": v})
            good_vals = coerced[~bad_type]
            minv, maxv = meta.get("minValue"), meta.get("maxValue")
            if minv is not None:
                mask = good_vals < minv
                if mask.any():
                    n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[mask].items():
                        errors.append({"row_index": int(i), "column": col, "issue": f"< minValue {minv}", "value": int(v)})
            if maxv is not None:
                mask = good_vals > maxv
                if mask.any():
                    n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[mask].items():
                        errors.append({"row_index": int(i), "column": col, "issue": f"> maxValue {maxv}", "value": int(v)})

        # FLOAT
        elif typ in ("FLOAT", "DOUBLE", "DECIMAL", "NUMERIC"):
            coerced = pd.to_numeric(nn, errors="coerce")
            bad_type = coerced.isna()
            if bad_type.any():
                n_bad = int(bad_type.sum()); col_summary[col]["errors"] += n_bad
                for i, v in nn[bad_type].items():
                    errors.append({"row_index": int(i), "column": col, "issue": "Expected FLOAT", "value": v})
            good_vals = coerced[~bad_type]
            minv, maxv = meta.get("minValue"), meta.get("maxValue")
            if minv is not None:
                mask = good_vals < float(minv)
                if mask.any():
                    n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[mask].items():
                        errors.append({"row_index": int(i), "column": col, "issue": f"< minValue {minv}", "value": float(v)})
            if maxv is not None:
                mask = good_vals > float(maxv)
                if mask.any():
                    n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[mask].items():
                        errors.append({"row_index": int(i), "column": col, "issue": f"> maxValue {maxv}", "value": float(v)})

        # DATE / DATETIME / TIMESTAMP
        elif typ in ("DATE", "DATETIME", "TIMESTAMP"):
            # Robust parse to UTC-aware timestamps
            parsed_utc = parse_datetime_flex(nn)
            bad_type = parsed_utc.isna()
            if bad_type.any():
                n_bad = int(bad_type.sum()); col_summary[col]["errors"] += n_bad
                for i, v in nn[bad_type].items():
                    errors.append({
                        "row_index": int(i), "column": col,
                        "issue": "Unparseable datetime (ISO 8601 expected)",
                        "value": v
                    })

            good_vals = parsed_utc[~bad_type]

            # Choose comparison mode: per-column override or global default
            mode = meta.get("dateCompareMode") or _DATE_COMPARE_MODE
            if mode not in ("datetime", "date"):
                mode = "datetime"

            # Parse min/max; accept simple dates or datetimes
            minv, maxv = meta.get("minValue"), meta.get("maxValue")
            if minv is not None:
                min_parsed = pd.to_datetime(pd.Series([minv]), errors="coerce", utc=True).iloc[0]
                if pd.isna(min_parsed):
                    # try legacy plain date formats as fallback
                    min_parsed = _try_parse_dates(pd.Series([minv])).iloc[0]
                    if pd.notna(min_parsed):
                        min_parsed = min_parsed.tz_localize("UTC")
            else:
                min_parsed = pd.NaT

            if maxv is not None:
                max_parsed = pd.to_datetime(pd.Series([maxv]), errors="coerce", utc=True).iloc[0]
                if pd.isna(max_parsed):
                    max_parsed = _try_parse_dates(pd.Series([maxv])).iloc[0]
                    if pd.notna(max_parsed):
                        max_parsed = max_parsed.tz_localize("UTC")
            else:
                max_parsed = pd.NaT

            if mode == "date":
                good_cmp = coerce_to_date(good_vals)
                min_cmp = coerce_to_date(pd.Series([min_parsed])).iloc[0] if pd.notna(min_parsed) else None
                max_cmp = coerce_to_date(pd.Series([max_parsed])).iloc[0] if pd.notna(max_parsed) else None
            else:
                good_cmp = good_vals
                min_cmp = min_parsed if pd.notna(min_parsed) else None
                max_cmp = max_parsed if pd.notna(max_parsed) else None

            # Bounds checks
            if min_cmp is not None:
                oob = good_cmp < min_cmp
                if oob.any():
                    n_bad = int(oob.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[oob].items():
                        errors.append({
                            "row_index": int(i), "column": col,
                            "issue": f"< minValue {minv}",
                            "value": v.isoformat()
                        })
            if max_cmp is not None:
                oob = good_cmp > max_cmp
                if oob.any():
                    n_bad = int(oob.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[oob].items():
                        errors.append({
                            "row_index": int(i), "column": col,
                            "issue": f"> maxValue {maxv}",
                            "value": v.isoformat()
                        })

        # VARCHAR
        elif typ in ("VARCHAR", "CHAR"):
            s_str = nn.astype("string")
            cat_ratios = meta.get("categoryRatios")
            if isinstance(cat_ratios, dict) and len(cat_ratios) > 0:
                allowed = set(cat_ratios.keys())
                bad = ~s_str.isin(allowed)
                if bad.any():
                    n_bad = int(bad.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[bad].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": "Value not in allowed categories", "value": v})
            max_len = declared_max_len if declared_max_len is not None else meta.get("maxCharLength")
            if isinstance(max_len, int):
                too_long = s_str.str.len() > max_len
                if too_long.any():
                    n_bad = int(too_long.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[too_long].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": f"Length > {max_len}", "value": v})

        # TEXT
        elif typ == "TEXT":
            s_str = nn.astype("string")
            min_len, max_len = meta.get("minCharLength"), meta.get("maxCharLength")
            if isinstance(min_len, int):
                too_short = s_str.str.len() < min_len
                if too_short.any():
                    n_bad = int(too_short.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[too_short].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": f"Length < {min_len}", "value": v})
            if isinstance(max_len, int):
                too_long = s_str.str.len() > max_len
                if too_long.any():
                    n_bad = int(too_long.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[too_long].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": f"Length > {max_len}", "value": v})

        else:
            # Unknown datatype â€” informational note once per column (only if data present)
            if len(nn) > 0:
                errors.append({"row_index": -1, "column": col,
                               "issue": f"Unrecognized datatype '{typ}' (skipped strict checks)",
                               "value": None})

    # Assemble outputs
    errors_df = pd.DataFrame(errors, columns=["row_index", "column", "issue", "value"]) \
                   .sort_values(["column", "row_index"], ignore_index=True)

    # Build reasons & final validity
    reasons = []
    if missing_required:
        reasons.append(f"Missing required columns: {missing_required}")
    if not errors_df.empty:
        reasons.append(f"Row-level errors: {len(errors_df)}")
    if fail_on_unexpected and len(unexpected) > 0:
        reasons.append(f"Unexpected columns: {unexpected}")

    summary = {
        "valid": (len(missing_required) == 0)
                 and errors_df.empty
                 and (not fail_on_unexpected or len(unexpected) == 0),
        "reasons": reasons,
        "missing_required_columns": missing_required,
        "unexpected_columns": unexpected,
        "columns": col_summary,
        "overlap_columns": sorted(list(ddl_cols & df_cols)),
        "ddl_only_columns": sorted(list(ddl_cols - df_cols)),
        "df_only_columns": sorted(list(df_cols - ddl_cols)),
    }

    return errors_df, summary

# -------------------- Inline usage example (comment/uncomment) --------------------
# ddl = load_ddl("ddl.json")
# df = your_dataframe  # e.g., pd.read_parquet("events.parquet")
# _DATE_COMPARE_MODE = "date"  # uncomment if you want date-only comparison globally
# errors_df, summary = validate_df_against_ddl(
#     df, ddl,
#     fail_on_unexpected=False,
#     normalize_column_names=True
# )
# print("Valid:", summary["valid"])
# print("Reasons:", summary["reasons"])
# print("Missing required:", summary["missing_required_columns"])
# print("Unexpected:", summary["unexpected_columns"])
# display(errors_df.head(20))
