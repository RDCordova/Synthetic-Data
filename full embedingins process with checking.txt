import json
import boto3
import time
import pandas as pd
import cudf  # GPU-accelerated Pandas
import faiss
import numpy as np
from datetime import datetime, timedelta

# Initialize Amazon Bedrock client
bedrock = boto3.client("bedrock-runtime")

# Constants
MAX_TOKENS = 8000  # Safe margin below Titan's 8192-token limit
OVERLAP_TOKENS = 2000  # Overlapping context window
BATCH_SIZE = 5  # Adjust for best performance

### **Step 1: Load and Chunk Logs (Time-Based)**
def load_logs(json_file):
    """Loads system logs into a GPU-accelerated DataFrame (cuDF)."""
    with open(json_file, "r") as f:
        data = json.load(f)
    
    df = cudf.DataFrame(data["logs"])
    df["timestamp"] = cudf.to_datetime(df["_time"])  # Convert timestamps
    return df

def chunk_logs_with_max_length(df, window_minutes=10):
    """Chunks logs into time-based groups while ensuring each chunk stays within max_length."""
    df = df.sort_values("timestamp")
    df["chunk_id"] = (df["timestamp"] - df["timestamp"].min()) // timedelta(minutes=window_minutes)

    # Handle None values
    df["CWmessage"] = df["CWmessage"].fillna("")

    final_chunks = []
    current_chunk = []
    current_length = 0
    last_timestamp = None
    chunk_id = 0

    for _, row in df.iterrows():
        message = f"[{row['timestamp']}] {row['CWmessage']}\n"
        message_length = len(message)

        # If adding this message exceeds max_length, store current chunk and start a new one
        if current_length + message_length > MAX_TOKENS * 4:  # Approximate char limit
            final_chunks.append({"chunk_id": chunk_id, "timestamp": last_timestamp, "formatted_logs": "".join(current_chunk)})
            chunk_id += 1  # Increment chunk ID
            current_chunk = []  # Reset chunk
            current_length = 0  # Reset length

        # Add message to chunk
        current_chunk.append(message)
        current_length += message_length
        last_timestamp = row["timestamp"]

    # Append the last remaining chunk
    if current_chunk:
        final_chunks.append({"chunk_id": chunk_id, "timestamp": last_timestamp, "formatted_logs": "".join(current_chunk)})

    return pd.DataFrame(final_chunks)  # Convert to Pandas for further processing

### **Step 2: Ensure Logs Fit Titan's 8192-Token Limit**
def validate_titan_token_count(text):
    """Use Titan to check token count before sending for embedding."""
    response = bedrock.invoke_model(
        modelId="amazon.titan-embed-text-v1",
        contentType="application/json",
        accept="application/json",
        body=json.dumps({"inputText": text, "returnTokenCount": True})  # Ask Titan to return token count
    )
    return json.loads(response["body"].read())["tokenCount"]

def reduce_text_to_token_limit(text, max_tokens=MAX_TOKENS):
    """Dynamically reduce text until it fits within Titan's token limit."""
    while validate_titan_token_count(text) > max_tokens:
        text = " ".join(text.split()[:int(len(text.split()) * 0.8)])  # Reduce by 20%
    return text

def split_text_into_token_chunks(text, max_tokens=MAX_TOKENS, overlap_tokens=OVERLAP_TOKENS):
    """Split large text into overlapping chunks while staying within token limits."""
    words = text.split()
    chunks = []
    start = 0
    chunk_count = 0
    start_time = time.time()

    while start < len(words):
        end = min(start + max_tokens, len(words))
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start = end - overlap_tokens  # Overlap ensures no data loss
        chunk_count += 1

    elapsed_time = time.time() - start_time  # Calculate time taken
    print(f"Split text into {chunk_count} chunks | Time Taken: {elapsed_time:.2f}s | Original Length: {len(words)} tokens")

    return chunks

### **Step 3: Generate Embeddings with Titan (Batch Processing + Tracking)**
def get_titan_embeddings_batch(texts):
    """Generate embeddings for a batch of texts using Amazon Titan with token-safe sliding windows."""
    batched_embeddings = []
    total_batches = len(texts) // BATCH_SIZE + (1 if len(texts) % BATCH_SIZE != 0 else 0)

    start_time = time.time()

    for i in range(0, len(texts), BATCH_SIZE):
        batch = texts[i:i+BATCH_SIZE]
        embeddings = []
        batch_start_time = time.time()

        for text in batch:
            token_count = validate_titan_token_count(text)

            # Reduce dynamically if too large
            if token_count > MAX_TOKENS:
                text = reduce_text_to_token_limit(text, MAX_TOKENS)

            response = bedrock.invoke_model(
                modelId="amazon.titan-embed-text-v1",
                contentType="application/json",
                accept="application/json",
                body=json.dumps({"inputText": text})
            )
            embedding = json.loads(response["body"].read())["embedding"]
            embeddings.append(embedding)

        batched_embeddings.extend(embeddings)

        batch_time = time.time() - batch_start_time
        elapsed_time = time.time() - start_time
        avg_time_per_batch = elapsed_time / (i // BATCH_SIZE + 1)
        remaining_batches = total_batches - (i // BATCH_SIZE + 1)
        estimated_remaining_time = remaining_batches * avg_time_per_batch

        print(f"Processed {i+len(batch)}/{len(texts)} embeddings | Batch Time: {batch_time:.2f}s | Estimated Remaining: {estimated_remaining_time:.2f}s")

    return batched_embeddings

### **Step 4: Store Embeddings in FAISS (GPU-Accelerated)**
def store_embeddings_in_faiss(df):
    """Stores embeddings in FAISS using GPU acceleration."""
    embeddings = np.vstack(df["embedding"].values).astype("float32")

    # Move FAISS to GPU
    index = faiss.IndexFlatL2(embeddings.shape[1])
    res = faiss.StandardGpuResources()
    gpu_index = faiss.index_cpu_to_gpu(res, 0, index)
    gpu_index.add(embeddings)

    # Save FAISS index
    faiss.write_index(gpu_index, "faiss_gpu_index.idx")
    df.to_parquet("chunked_log_embeddings.parquet", index=False)

    print("✅ Embeddings stored in FAISS-GPU and saved successfully.")

### **Step 5: Full Pipeline Execution**
# Load and process logs
json_file = "system_logs.json"
df_logs = load_logs(json_file)

# Apply time-based chunking
df_chunked = chunk_logs_with_max_length(df_logs, window_minutes=10)

# Apply token-based splitting if needed
df_chunked["split_messages"] = df_chunked["formatted_logs"].apply(
    lambda text: split_text_into_token_chunks(text) if validate_titan_token_count(text) > MAX_TOKENS else [text]
)

# Flatten dataset
df_expanded = df_chunked.explode("split_messages").reset_index(drop=True)
df_expanded.rename(columns={"split_messages": "formatted_logs"}, inplace=True)

# Generate embeddings
df_expanded["embedding"] = get_titan_embeddings_batch(df_expanded["formatted_logs"].tolist())

# Store embeddings in FAISS-GPU
store_embeddings_in_faiss(df_expanded)

print("✅ Full embedding pipeline completed successfully.")
