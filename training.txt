import faiss
import pandas as pd
import os
import json
import numpy as np
import boto3
from datetime import datetime

# File paths
FAISS_INDEX_PATH = "faiss_index.idx"
EMBEDDING_DATA_PATH = "chunked_log_embeddings.parquet"
CSV_PATH = "expected_monthly_events.csv"
NEW_LOGS_STORAGE_PATH = "new_logs.parquet"

# Initialize Amazon Bedrock client
bedrock = boto3.client("bedrock-runtime", region_name="us-east-1")
EMBED_MODEL_ID = "amazon.titan-embed-text-v1"
CLAUDE_MODEL_ID = "anthropic.claude-3-5-sonnet-20240620"

# Load FAISS Index and Historical Logs
def load_faiss_index():
    """Loads FAISS index and historical log embeddings."""
    cpu_index = faiss.read_index(FAISS_INDEX_PATH)
    df = pd.read_parquet(EMBEDDING_DATA_PATH)
    return cpu_index, df

# Load expected monthly events from CSV
def load_expected_events():
    """Load expected monthly events from a CSV file."""
    df = pd.read_csv(CSV_PATH)
    return df.to_dict(orient="records")

# Load new logs from storage
def load_new_logs():
    """Load stored new logs or initialize an empty DataFrame."""
    if os.path.exists(NEW_LOGS_STORAGE_PATH):
        return pd.read_parquet(NEW_LOGS_STORAGE_PATH)
    else:
        return pd.DataFrame(columns=["timestamp", "log_message"])

# Save new logs to storage
def save_new_logs(df):
    """Save new logs to persistent storage."""
    df.to_parquet(NEW_LOGS_STORAGE_PATH, index=False)

# Load all data
faiss_index, df_logs = load_faiss_index()
EXPECTED_MONTHLY_EVENTS = load_expected_events()
df_new_logs = load_new_logs()

print(f"âœ… Loaded {len(df_logs)} historical log embeddings.")
print(f"âœ… Loaded {len(EXPECTED_MONTHLY_EVENTS)} expected monthly events.")
print(f"âœ… Loaded {len(df_new_logs)} previously seen logs.")

# ðŸš€ Step 1: Train Claude 3.5 on Historical Logs Using Chunks
def chunk_logs(df_logs, chunk_size=10000):
    """Splits the logs into smaller chunks to stay within Claude's input limit."""
    for i in range(0, len(df_logs), chunk_size):
        yield df_logs.iloc[i:i + chunk_size]

def train_claude_on_logs():
    """Train Claude 3.5 on logs in smaller chunks to avoid API limits."""
    
    # Sort logs by timestamp
    df_logs_sorted = df_logs.sort_values(by="timestamp")

    # Format expected monthly events
    expected_events_str = "\n".join([
        f"- Process: {event['process_name']}, Data Set ID: {event['data_set_id']}, Event Type: {event['event_type_name']}"
        for event in EXPECTED_MONTHLY_EVENTS
    ])

    # Store training results from each chunk
    training_results = []

    # Train on each chunk separately
    for chunk in chunk_logs(df_logs_sorted):
        
        # Convert formatted logs into structured text
        past_logs_str = "\n".join([
            f"[{row['timestamp']}] {row['formatted_logs']}"
            for _, row in chunk.iterrows()
        ])

        # Create structured training prompt
        training_prompt = f"""
        You are an AI analyzing system logs to detect anomalies.

        **1. Expected Monthly Events:**
        These events must occur each month. If they are missing, it indicates an issue:
        {expected_events_str}

        **2. Historical Log Data (Batch Processing):**
        Below is a batch of past system logs with timestamps:
        {past_logs_str}

        **Task:**
        - Identify common log patterns.
        - Explain the typical sequence of events.
        - Detect any inconsistencies or anomalies.
        - Compare past logs with the expected monthly events and highlight missing or duplicate occurrences.

        Provide your analysis in a structured format.
        """

        # Invoke Claude for each batch
        training_summary = invoke_claude(training_prompt)
        training_results.append(training_summary)

    # Combine results from all chunks
    final_training_summary = "\n\n".join(training_results)

    return final_training_summary

# Train Claude
training_summary = train_claude_on_logs()
print("\nðŸ”¹ Claude's Updated Training Summary:\n", training_summary)

# ðŸš€ Step 2: Process New Logs & Track Duplicates
def process_new_logs(new_logs):
    """Process new logs, check for duplicates, and update storage."""
    global df_new_logs

    # Convert new logs into DataFrame
    new_logs_df = pd.DataFrame(new_logs, columns=["timestamp", "log_message"])

    # Identify duplicate logs (semantic search will refine this)
    duplicates = new_logs_df[new_logs_df["log_message"].isin(df_new_logs["log_message"])]

    # Append only unique logs
    df_new_logs = pd.concat([df_new_logs, new_logs_df]).drop_duplicates().reset_index(drop=True)
    save_new_logs(df_new_logs)

    return duplicates

# ðŸš€ Step 3: FAISS Embedding & Semantic Search
def get_embedding(text):
    """Generate an embedding using Amazon Titan."""
    response = bedrock.invoke_model(
        modelId=EMBED_MODEL_ID,
        contentType="application/json",
        accept="application/json",
        body=json.dumps({"inputText": text})
    )
    return np.array(json.loads(response["body"].read())["embedding"]).astype("float32")

def search_faiss(query_embedding, top_k=5):
    """Search for similar embeddings in FAISS."""
    query_embedding = query_embedding.reshape(1, -1)
    distances, indices = faiss_index.search(query_embedding, top_k)
    return df_logs.iloc[indices[0]]

# ðŸš€ Step 4: Detect Anomalies Using Claude
def detect_anomalies_with_claude():
    """Analyze new logs and return structured results."""
    global df_new_logs

    # Convert logs into text format
    new_logs_str = "\n".join(df_new_logs["log_message"].tolist())

    # Create structured prompt
    anomaly_detection_prompt = f"""
    You are analyzing system logs for anomalies.

    **1. Expected Monthly Events:**
    {EXPECTED_MONTHLY_EVENTS}

    **2. New Logs for This Month:**
    {new_logs_str}

    **3. Previously Learned Log Patterns:**
    {training_summary}

    **Task:**
    - Compare new logs against historical patterns.
    - Detect duplicate events.
    - Identify missing expected events.
    - Provide a structured JSON output.
    """

    # Invoke Claude
    response = invoke_claude(anomaly_detection_prompt)
    return json.loads(response)

# ðŸš€ Step 5: Generate Final Report
def generate_final_report(new_logs):
    """Run anomaly detection and compile results."""
    
    # Detect anomalies
    anomalies_output = detect_anomalies_with_claude()

    # Compile final structured report
    final_report = {
        "anomalies_found": anomalies_output["anomalies_found"],
        "duplicate_files_found": anomalies_output["duplicate_files_found"],
        "missing_events": anomalies_output["missing_events"]
    }

    print("\nðŸ”¹ Final Anomaly Detection Report:\n", json.dumps(final_report, indent=4))
    return final_report

# Process new logs and generate report
new_logs_today = [
    ("2025-03-11 10:15:00", "File upload: sales_data_2025_03.csv"),
    ("2025-03-11 10:20:00", "Processing error detected for sales_data_2025_03.csv"),
    ("2025-03-11 10:30:00", "Backup completed for sales_data_2025_03.csv")
]

process_new_logs(new_logs_today)
final_report = generate_final_report([log[1] for log in new_logs_today])
