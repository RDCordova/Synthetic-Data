import json
import boto3
import time
import pandas as pd
import cudf  # GPU-accelerated Pandas
import faiss
import numpy as np
from datetime import datetime, timedelta

# Initialize Amazon Bedrock client
bedrock = boto3.client("bedrock-runtime")

# **New Safe Character Limits**
MAX_CHARACTERS = 16000  # Conservative limit (well below Titan's 8192-token max)
OVERLAP_CHARACTERS = 4000  # Overlap to retain context
BATCH_SIZE = 5  # Adjust for best performance

### **Step 1: Load and Chunk Logs (Time-Based)**
def load_logs(json_file):
    """Loads system logs into a GPU-accelerated DataFrame (cuDF)."""
    with open(json_file, "r") as f:
        data = json.load(f)
    
    df = cudf.DataFrame(data["logs"])
    df["timestamp"] = cudf.to_datetime(df["_time"])  # Convert timestamps
    return df

def chunk_logs_with_max_length(df, window_minutes=10):
    """Chunks logs into time-based groups while ensuring each chunk stays within max_length."""
    df = df.sort_values("timestamp")
    df["chunk_id"] = (df["timestamp"] - df["timestamp"].min()) // timedelta(minutes=window_minutes)

    # Handle None values
    df["CWmessage"] = df["CWmessage"].fillna("")

    final_chunks = []
    current_chunk = []
    current_length = 0
    last_timestamp = None
    chunk_id = 0

    for _, row in df.iterrows():
        message = f"[{row['timestamp']}] {row['CWmessage']}\n"
        message_length = len(message)

        # If adding this message exceeds max_length, store current chunk and start a new one
        if current_length + message_length > MAX_CHARACTERS:  # New Safe Character Limit
            final_chunks.append({"chunk_id": chunk_id, "timestamp": last_timestamp, "formatted_logs": "".join(current_chunk)})
            chunk_id += 1  # Increment chunk ID
            current_chunk = []  # Reset chunk
            current_length = 0  # Reset length

        # Add message to chunk
        current_chunk.append(message)
        current_length += message_length
        last_timestamp = row["timestamp"]

    # Append the last remaining chunk
    if current_chunk:
        final_chunks.append({"chunk_id": chunk_id, "timestamp": last_timestamp, "formatted_logs": "".join(current_chunk)})

    return pd.DataFrame(final_chunks)  # Convert to Pandas for further processing

### **Step 2: Simple Character-Based Splitting (New Approach)**
def split_large_text(text, max_length=MAX_CHARACTERS, overlap=OVERLAP_CHARACTERS):
    """Splits a large text into smaller overlapping chunks based on characters, not tokens."""
    chunks = []
    start = 0

    while start < len(text):
        end = min(start + max_length, len(text))
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap  # Overlapping chunks retain context

    return chunks

### **Step 3: Generate Embeddings with Titan (Batch Processing + Tracking)**
def get_titan_embeddings_batch(texts):
    """Generate embeddings for a batch of texts using Amazon Titan with character-based splitting."""
    batched_embeddings = []
    total_batches = len(texts) // BATCH_SIZE + (1 if len(texts) % BATCH_SIZE != 0 else 0)

    start_time = time.time()

    for i in range(0, len(texts), BATCH_SIZE):
        batch = texts[i:i+BATCH_SIZE]
        embeddings = []
        batch_start_time = time.time()

        for text in batch:
            response = bedrock.invoke_model(
                modelId="amazon.titan-embed-text-v1",
                contentType="application/json",
                accept="application/json",
                body=json.dumps({"inputText": text})
            )
            embedding = json.loads(response["body"].read())["embedding"]
            embeddings.append(embedding)

        batched_embeddings.extend(embeddings)

        batch_time = time.time() - batch_start_time
        elapsed_time = time.time() - start_time
        avg_time_per_batch = elapsed_time / (i // BATCH_SIZE + 1)
        remaining_batches = total_batches - (i // BATCH_SIZE + 1)
        estimated_remaining_time = remaining_batches * avg_time_per_batch

        print(f"Processed {i+len(batch)}/{len(texts)} embeddings | Batch Time: {batch_time:.2f}s | Estimated Remaining: {estimated_remaining_time:.2f}s")

    return batched_embeddings

### **Step 4: Store Embeddings in FAISS (GPU-Accelerated)**
def store_embeddings_in_faiss(df):
    """Stores embeddings in FAISS using GPU acceleration."""
    embeddings = np.vstack(df["embedding"].values).astype("float32")

    # Move FAISS to GPU
    index = faiss.IndexFlatL2(embeddings.shape[1])
    res = faiss.StandardGpuResources()
    gpu_index = faiss.index_cpu_to_gpu(res, 0, index)
    gpu_index.add(embeddings)

    # Save FAISS index
    faiss.write_index(gpu_index, "faiss_gpu_index.idx")
    df.to_parquet("chunked_log_embeddings.parquet", index=False)

    print("✅ Embeddings stored in FAISS-GPU and saved successfully.")

### **Step 5: Full Pipeline Execution**
# Load and process logs
json_file = "system_logs.json"
df_logs = load_logs(json_file)

# Apply time-based chunking
df_chunked = chunk_logs_with_max_length(df_logs, window_minutes=10)

# Apply character-based splitting (Simple & Safe)
df_chunked["split_messages"] = df_chunked["formatted_logs"].apply(split_large_text)

# Flatten dataset
df_expanded = df_chunked.explode("split_messages").reset_index(drop=True)
df_expanded.rename(columns={"split_messages": "formatted_logs"}, inplace=True)

# Generate embeddings
df_expanded["embedding"] = get_titan_embeddings_batch(df_expanded["formatted_logs"].to_pandas().tolist())

# Store embeddings in FAISS-GPU
store_embeddings_in_faiss(df_expanded)

print("✅ Full embedding pipeline completed successfully.")
