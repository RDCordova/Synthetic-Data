from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import matplotlib.pyplot as plt
import logging
import time

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def log_step(step_description):
    """Logs and prints the progress."""
    logging.info(step_description)
    print(f"âœ” {step_description}")

# Start time tracking
start_time = time.time()

# ðŸš€ Step 1: Initialize Spark and Load Data
log_step("Initializing Spark Session...")
spark = SparkSession.builder.appName("KMeans_Clustering").getOrCreate()

log_step("Loading raw data from Parquet...")
df_spark = spark.read.parquet("your_raw_data.parquet")

# Display schema to check raw features
df_spark.printSchema()

# ðŸš€ Step 2: Identify Categorical and Numerical Columns
categorical_cols = [col for col, dtype in df_spark.dtypes if dtype == 'string']
numerical_cols = [col for col, dtype in df_spark.dtypes if dtype in ('int', 'double', 'float')]

log_step(f"Identified Categorical Columns: {categorical_cols}")
log_step(f"Identified Numerical Columns: {numerical_cols}")

# ðŸš€ Step 3: Encode Categorical Features
log_step("Encoding categorical features using StringIndexer...")

indexers = [StringIndexer(inputCol=col, outputCol=col+"_index").fit(df_spark) for col in categorical_cols]
for indexer in indexers:
    df_spark = indexer.transform(df_spark)

# ðŸš€ Step 4: Scale Numerical Features
log_step("Scaling numerical features using StandardScaler...")

feature_columns = numerical_cols + [col+"_index" for col in categorical_cols]
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features_unscaled")
df_spark = assembler.transform(df_spark)

scaler = StandardScaler(inputCol="features_unscaled", outputCol="features", withStd=True, withMean=True)
df_spark = scaler.fit(df_spark).transform(df_spark)

# ðŸš€ Step 5: Determine Optimal k (Elbow & Silhouette)
log_step("Determining the optimal number of clusters (k)...")

costs = []
silhouette_scores = []
K_range = range(2, 10)
best_k = 2
best_score = -1

for k in K_range:
    log_step(f"Training K-Means for k={k}...")
    
    kmeans = KMeans(featuresCol="features", k=k, seed=42)
    model = kmeans.fit(df_spark)

    # Elbow Method (Inertia)
    inertia = model.summary.trainingCost
    costs.append(inertia)

    # Silhouette Score
    predictions = model.transform(df_spark)
    evaluator = ClusteringEvaluator(featuresCol="features", metricName="silhouette")
    silhouette_score = evaluator.evaluate(predictions)
    silhouette_scores.append(silhouette_score)

    log_step(f"  â†’ Inertia: {inertia:.2f}, Silhouette Score: {silhouette_score:.4f}")

    if silhouette_score > best_score:
        best_score = silhouette_score
        best_k = k

log_step(f"Optimal k based on Silhouette Score: {best_k}")

# ðŸš€ Step 6: Plot Elbow Method
log_step("Plotting the Elbow Method curve...")
plt.figure(figsize=(8, 5))
plt.plot(K_range, costs, marker='o', linestyle='--')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Cost)')
plt.title('Elbow Method for Optimal k')
plt.show()

# ðŸš€ Step 7: Train Final K-Means Model
log_step(f"Training final K-Means model with k={best_k}...")
kmeans = KMeans(featuresCol="features", k=best_k, seed=42)
model = kmeans.fit(df_spark)

# ðŸš€ Step 8: Assign Clusters
log_step("Assigning clusters to dataset...")
df_spark = model.transform(df_spark)

# ðŸš€ Step 9: Save Clustered Data
log_step("Saving clustered data to Parquet...")
df_spark.write.mode("overwrite").parquet("clustered_data.parquet")

# ðŸš€ Step 10: Display Results
log_step("Fetching clustered data for visualization...")
df_pandas = df_spark.select("Cluster").toPandas()

import ace_tools as tools
tools.display_dataframe_to_user(name="Clustered Data", dataframe=df_pandas)

# ðŸš€ Final Step: Process Complete!
end_time = time.time()
log_step(f"Clustering process completed in {round(end_time - start_time, 2)} seconds! ðŸŽ‰")
