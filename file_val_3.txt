# -*- coding: utf-8 -*-
# -*- coding: utf-8 -*-
"""
DataFrame Validator against JSON DDL
- Reads DDL from a JSON file
- Validates presence, nullability, types, ranges, lengths, categories
- Returns row-level errors and a detailed summary (with reasons)
"""

import json
from typing import Dict, List
import pandas as pd
import numpy as np

# Edit as needed
DATE_FORMATS = ["%Y-%m-%d", "%Y%m%d", "%Y/%m/%d", "%m/%d/%Y"]

# ---------- I/O ----------

def load_ddl(path: str) -> dict:
    """Load the DDL JSON file and return it as a Python dict."""
    with open(path, "r") as f:
        data = f.read()          # read as string
    return json.loads(data)      # parse into dict

# ---------- Helpers ----------

def _try_parse_dates(series: pd.Series, formats: List[str] = DATE_FORMATS) -> pd.Series:
    """Coerce strings â†’ datetime using multiple formats; preserves NaN."""
    s = series.astype("string")
    out = pd.Series(pd.NaT, index=s.index, dtype="datetime64[ns]")
    remaining = s.notna()

    for fmt in formats:
        if not remaining.any():
            break
        parsed = pd.to_datetime(s[remaining], format=fmt, errors="coerce")
        good = parsed.notna()
        good_idx = parsed.index[good]
        out.loc[good_idx] = parsed.loc[good_idx]
        remaining.loc[good_idx] = False

    return out

def _ddl_columns_by_name(ddl: dict, normalize: bool = False) -> Dict[str, dict]:
    """Flatten DDL to {column_name: column_def}. Optionally strip names."""
    by_name = {}
    for block in ddl.get("ddl", []):
        for c in block.get("columns", []):
            name = c["name"]
            if normalize and isinstance(name, str):
                name = name.strip()
            by_name[name] = c
    return by_name

# ---------- Core Validator ----------

def validate_df_against_ddl(
    df: pd.DataFrame,
    ddl: dict,
    *,
    fail_on_unexpected: bool = False,
    normalize_column_names: bool = False
):
    """
    Validate df against ddl.

    Returns:
      errors_df: DataFrame[row_index, column, issue, value]
      summary: {
        valid: bool,
        reasons: List[str],
        missing_required_columns: List[str],
        unexpected_columns: List[str],
        columns: Dict[col, {"errors": int, "nulls": int|None}],
        overlap_columns: List[str],
        ddl_only_columns: List[str],
        df_only_columns: List[str],
      }
    """
    # Optional normalization of column names
    if normalize_column_names:
        df = df.rename(columns=lambda c: c.strip() if isinstance(c, str) else c)

    by_name = _ddl_columns_by_name(ddl, normalize=normalize_column_names)
    ddl_cols = set(by_name.keys())
    df_cols = set(df.columns)

    required = {c for c, d in by_name.items()
                if not (d.get("type", {}) or {}).get("nullable", True)}
    missing_required = sorted(list(required - df_cols))
    unexpected = sorted(list(df_cols - ddl_cols))

    col_summary = {
        c: {"errors": 0, "nulls": (int(df[c].isna().sum()) if c in df_cols else None)}
        for c in ddl_cols
    }

    errors = []

    # Validate only overlapping columns
    for col in sorted(ddl_cols & df_cols):
        spec = by_name[col]
        typ = (spec.get("type", {}) or {}).get("datatype", "").upper()
        nullable = (spec.get("type", {}) or {}).get("nullable", True)
        declared_max_len = (spec.get("type", {}) or {}).get("maxLength", None)
        meta = spec.get("metadata", {}) or {}

        s = df[col]
        null_mask = s.isna() | (s.astype("string").str.strip() == "")

        # Nullability
        if not nullable and null_mask.any():
            n_bad = int(null_mask.sum())
            col_summary[col]["errors"] += n_bad
            for i in s.index[null_mask]:
                errors.append({"row_index": int(i), "column": col,
                               "issue": "Null/empty but not nullable", "value": None})

        nn = s[~null_mask]

        # INT
        if typ in ("INT", "INTEGER"):
            coerced = pd.to_numeric(nn, errors="coerce", downcast="integer")
            bad_type = coerced.isna()
            if bad_type.any():
                n_bad = int(bad_type.sum()); col_summary[col]["errors"] += n_bad
                for i, v in nn[bad_type].items():
                    errors.append({"row_index": int(i), "column": col, "issue": "Expected INT", "value": v})
            good_vals = coerced[~bad_type]
            minv, maxv = meta.get("minValue"), meta.get("maxValue")
            if minv is not None:
                mask = good_vals < minv
                if mask.any():
                    n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[mask].items():
                        errors.append({"row_index": int(i), "column": col, "issue": f"< minValue {minv}", "value": int(v)})
            if maxv is not None:
                mask = good_vals > maxv
                if mask.any():
                    n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[mask].items():
                        errors.append({"row_index": int(i), "column": col, "issue": f"> maxValue {maxv}", "value": int(v)})

        # FLOAT
        elif typ in ("FLOAT", "DOUBLE", "DECIMAL", "NUMERIC"):
            coerced = pd.to_numeric(nn, errors="coerce")
            bad_type = coerced.isna()
            if bad_type.any():
                n_bad = int(bad_type.sum()); col_summary[col]["errors"] += n_bad
                for i, v in nn[bad_type].items():
                    errors.append({"row_index": int(i), "column": col, "issue": "Expected FLOAT", "value": v})
            good_vals = coerced[~bad_type]
            minv, maxv = meta.get("minValue"), meta.get("maxValue")
            if minv is not None:
                mask = good_vals < float(minv)
                if mask.any():
                    n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[mask].items():
                        errors.append({"row_index": int(i), "column": col, "issue": f"< minValue {minv}", "value": float(v)})
            if maxv is not None:
                mask = good_vals > float(maxv)
                if mask.any():
                    n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in good_vals[mask].items():
                        errors.append({"row_index": int(i), "column": col, "issue": f"> maxValue {maxv}", "value": float(v)})

        # DATE
        elif typ in ("DATE", "DATETIME", "TIMESTAMP"):
            parsed = nn if np.issubdtype(nn.dtype, np.datetime64) else _try_parse_dates(nn)
            bad_type = parsed.isna()
            if bad_type.any():
                n_bad = int(bad_type.sum()); col_summary[col]["errors"] += n_bad
                for i, v in nn[bad_type].items():
                    errors.append({"row_index": int(i), "column": col,
                                   "issue": f"Expected DATE ({DATE_FORMATS})", "value": v})
            good_vals = parsed[~bad_type]
            if meta.get("minValue") is not None:
                min_dt = _try_parse_dates(pd.Series([meta["minValue"]])).iloc[0]
                if pd.notna(min_dt):
                    mask = good_vals < min_dt
                    if mask.any():
                        n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                        for i, v in good_vals[mask].items():
                            errors.append({"row_index": int(i), "column": col,
                                           "issue": f"< minValue {meta['minValue']}", "value": str(v.date())})
            if meta.get("maxValue") is not None:
                max_dt = _try_parse_dates(pd.Series([meta["maxValue"]])).iloc[0]
                if pd.notna(max_dt):
                    mask = good_vals > max_dt
                    if mask.any():
                        n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                        for i, v in good_vals[mask].items():
                            errors.append({"row_index": int(i), "column": col,
                                           "issue": f"> maxValue {meta['maxValue']}", "value": str(v.date())})

        # VARCHAR
        elif typ in ("VARCHAR", "CHAR"):
            s_str = nn.astype("string")
            cat_ratios = meta.get("categoryRatios")
            if isinstance(cat_ratios, dict) and len(cat_ratios) > 0:
                allowed = set(cat_ratios.keys())
                bad = ~s_str.isin(allowed)
                if bad.any():
                    n_bad = int(bad.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[bad].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": "Value not in allowed categories", "value": v})
            max_len = declared_max_len if declared_max_len is not None else meta.get("maxCharLength")
            if isinstance(max_len, int):
                too_long = s_str.str.len() > max_len
                if too_long.any():
                    n_bad = int(too_long.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[too_long].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": f"Length > {max_len}", "value": v})

        # TEXT
        elif typ == "TEXT":
            s_str = nn.astype("string")
            min_len, max_len = meta.get("minCharLength"), meta.get("maxCharLength")
            if isinstance(min_len, int):
                too_short = s_str.str.len() < min_len
                if too_short.any():
                    n_bad = int(too_short.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[too_short].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": f"Length < {min_len}", "value": v})
            if isinstance(max_len, int):
                too_long = s_str.str.len() > max_len
                if too_long.any():
                    n_bad = int(too_long.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[too_long].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": f"Length > {max_len}", "value": v})

        else:
            # Unknown datatype â€” informational note once per column (only if data present)
            if len(nn) > 0:
                errors.append({"row_index": -1, "column": col,
                               "issue": f"Unrecognized datatype '{typ}' (skipped strict checks)",
                               "value": None})

    # Assemble outputs
    errors_df = pd.DataFrame(errors, columns=["row_index", "column", "issue", "value"]) \
                   .sort_values(["column", "row_index"], ignore_index=True)

    # Build reasons & final validity
    reasons = []
    if missing_required:
        reasons.append(f"Missing required columns: {missing_required}")
    if not errors_df.empty:
        reasons.append(f"Row-level errors: {len(errors_df)}")
    if fail_on_unexpected and len(unexpected) > 0:
        reasons.append(f"Unexpected columns: {unexpected}")

    summary = {
        "valid": (len(missing_required) == 0)
                 and errors_df.empty
                 and (not fail_on_unexpected or len(unexpected) == 0),
        "reasons": reasons,
        "missing_required_columns": missing_required,
        "unexpected_columns": unexpected,
        "columns": col_summary,
        "overlap_columns": sorted(list(ddl_cols & df_cols)),
        "ddl_only_columns": sorted(list(ddl_cols - df_cols)),
        "df_only_columns": sorted(list(df_cols - ddl_cols)),
    }

    return errors_df, summary

# ---------- Inline usage example (edit paths/df as needed) ----------

# ddl = load_ddl("ddl.json")
# df = your_dataframe  # e.g., pd.read_parquet(...)

# errors_df, summary = validate_df_against_ddl(
#     df, ddl,
#     fail_on_unexpected=False,
#     normalize_column_names=True
# )

# print("Valid:", summary["valid"])
# print("Reasons:", summary["reasons"])
# print("Missing required:", summary["missing_required_columns"])
# print("Unexpected:", summary["unexpected_columns"])
# display(errors_df.head(20))

