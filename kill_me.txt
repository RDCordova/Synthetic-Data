import json
import boto3
import time
import pandas as pd
import cudf  # GPU-accelerated Pandas
import faiss
import numpy as np
from datetime import datetime, timedelta

# Initialize Amazon Bedrock client
bedrock = boto3.client("bedrock-runtime")

# **New Safe Character Limits**
MAX_CHARACTERS = 16000  # Conservative limit (well below Titan's 8192-token max)
OVERLAP_CHARACTERS = 4000  # Overlap to retain context
BATCH_SIZE = 5  # Adjust for best performance

### **Step 1: Load and Chunk Logs (With First-Pass Splitting)**
def load_logs(json_file):
    """Loads system logs into a GPU-accelerated DataFrame (cuDF)."""
    with open(json_file, "r") as f:
        data = json.load(f)
    
    df = cudf.DataFrame(data["logs"])
    df["timestamp"] = cudf.to_datetime(df["_time"])  # Convert timestamps
    return df

def chunk_logs_with_max_length(df, window_minutes=10, max_chars=16000, overlap_chars=4000):
    """Chunks logs into time-based groups while ensuring each chunk stays within Titan's character limit."""
    df = df.sort_values("timestamp")
    df["chunk_id"] = (df["timestamp"] - df["timestamp"].min()) // timedelta(minutes=window_minutes)

    # Handle None values
    df["CWmessage"] = df["CWmessage"].fillna("")

    final_chunks = []
    chunk_id = 0

    for _, row in df.iterrows():
        message = f"[{row['timestamp']}] {row['CWmessage']}\n"

        # **Split messages immediately if they exceed max_chars**
        if len(message) > max_chars:
            start = 0
            while start < len(message):
                end = min(start + max_chars, len(message))
                chunked_message = message[start:end]
                final_chunks.append({"chunk_id": chunk_id, "timestamp": row["timestamp"], "formatted_logs": chunked_message})
                start = end - overlap_chars  # Overlap ensures no data loss
                chunk_id += 1  # Unique ID for each split chunk
        else:
            final_chunks.append({"chunk_id": chunk_id, "timestamp": row["timestamp"], "formatted_logs": message})
            chunk_id += 1

    return pd.DataFrame(final_chunks)  # Convert to Pandas for further processing

### **Step 2: Generate Embeddings with Titan (Batch Processing + Tracking)**
def get_titan_embeddings_batch(texts):
    """Generate embeddings for a batch of texts using Amazon Titan with character-based splitting."""
    batched_embeddings = []
    total_batches = len(texts) // BATCH_SIZE + (1 if len(texts) % BATCH_SIZE != 0 else 0)

    start_time = time.time()

    for i in range(0, len(texts), BATCH_SIZE):
        batch = texts[i:i+BATCH_SIZE]
        embeddings = []
        batch_start_time = time.time()

        for text in batch:
            response = bedrock.invoke_model(
                modelId="amazon.titan-embed-text-v1",
                contentType="application/json",
                accept="application/json",
                body=json.dumps({"inputText": text})
            )
            embedding = json.loads(response["body"].read())["embedding"]
            embeddings.append(embedding)

        batched_embeddings.extend(embeddings)

        batch_time = time.time() - batch_start_time
        elapsed_time = time.time() - start_time
        avg_time_per_batch = elapsed_time / (i // BATCH_SIZE + 1)
        remaining_batches = total_batches - (i // BATCH_SIZE + 1)
        estimated_remaining_time = remaining_batches * avg_time_per_batch

        print(f"Processed {i+len(batch)}/{len(texts)} embeddings | Batch Time: {batch_time:.2f}s | Estimated Remaining: {estimated_remaining_time:.2f}s")

    return batched_embeddings

### **Step 3: Store Embeddings in FAISS (GPU-Accelerated)**
def store_embeddings_in_faiss(df):
    """Stores embeddings in FAISS using GPU acceleration."""
    embeddings = np.vstack(df["embedding"].values).astype("float32")

    # Move FAISS to GPU
    index = faiss.IndexFlatL2(embeddings.shape[1])
    res = faiss.StandardGpuResources()
    gpu_index = faiss.index_cpu_to_gpu(res, 0, index)
    gpu_index.add(embeddings)

    # Save FAISS index
    faiss.write_index(gpu_index, "faiss_gpu_index.idx")
    df.to_parquet("chunked_log_embeddings.parquet", index=False)

    print("✅ Embeddings stored in FAISS-GPU and saved successfully.")

### **Step 4: Full Pipeline Execution**
# Load and process logs
json_file = "system_logs.json"
df_logs = load_logs(json_file)

# Apply time-based chunking with built-in text splitting
df_chunked = chunk_logs_with_max_length(df_logs, window_minutes=10)

# Generate embeddings
df_chunked["embedding"] = get_titan_embeddings_batch(df_chunked["formatted_logs"].to_pandas().tolist())

# Store embeddings in FAISS-GPU
store_embeddings_in_faiss(df_chunked)

print("✅ Full embedding pipeline completed successfully.")

