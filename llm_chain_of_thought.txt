import faiss
import pandas as pd
import os
import json
import numpy as np
import boto3
from datetime import datetime

# File paths
FAISS_INDEX_PATH = "faiss_index.idx"
EMBEDDING_DATA_PATH = "chunked_log_embeddings.parquet"
CSV_PATH = "expected_monthly_events.csv"
NEW_LOGS_STORAGE_PATH = "new_logs.parquet"

# Initialize Amazon Bedrock client
bedrock = boto3.client("bedrock-runtime", region_name="us-east-1")
EMBED_MODEL_ID = "amazon.titan-embed-text-v1"
CLAUDE_MODEL_ID = "anthropic.claude-3-5-sonnet-20240620"

# Load FAISS Index and Historical Logs
def load_faiss_index():
    """Loads FAISS index and historical log embeddings."""
    cpu_index = faiss.read_index(FAISS_INDEX_PATH)
    df = pd.read_parquet(EMBEDDING_DATA_PATH)
    return cpu_index, df

# Load expected monthly events from CSV
def load_expected_events():
    """Load expected monthly events from a CSV file."""
    df = pd.read_csv(CSV_PATH)
    return df.to_dict(orient="records")

# Load new logs from storage
def load_new_logs():
    """Load stored new logs or initialize an empty DataFrame."""
    if os.path.exists(NEW_LOGS_STORAGE_PATH):
        return pd.read_parquet(NEW_LOGS_STORAGE_PATH)
    else:
        return pd.DataFrame(columns=["timestamp", "log_message"])

# Save new logs to storage
def save_new_logs(df):
    """Save new logs to persistent storage."""
    df.to_parquet(NEW_LOGS_STORAGE_PATH, index=False)

# Load all data
faiss_index, df_logs = load_faiss_index()
EXPECTED_MONTHLY_EVENTS = load_expected_events()
df_new_logs = load_new_logs()

print(f"âœ… Loaded {len(df_logs)} historical log embeddings.")
print(f"âœ… Loaded {len(EXPECTED_MONTHLY_EVENTS)} expected monthly events.")
print(f"âœ… Loaded {len(df_new_logs)} previously seen logs.")

# ðŸš€ Step 1: Train Claude 3.5 on Historical Logs & Expected Events
def train_claude_on_logs():
    """Train Claude 3.5 to understand log patterns and expected monthly events."""
    
    # Convert historical logs to string
    past_logs_str = "\n".join(df_logs["log_message"].tolist())

    # Format expected monthly events
    expected_events_str = "\n".join([
        f"- Process: {event['process_name']}, Data Set ID: {event['data_set_id']}, Event Type: {event['event_type_name']}"
        for event in EXPECTED_MONTHLY_EVENTS
    ])

    # Create structured training prompt
    training_prompt = f"""
    You are an AI analyzing system logs to detect anomalies.

    **1. Expected Monthly Events:**
    These events must occur each month. If they are missing, it indicates an issue:
    {expected_events_str}

    **2. Historical Log Data:**
    Below is a set of past system logs:
    {past_logs_str}

    **Task:**
    - Identify common log patterns.
    - Explain the typical sequence of events.
    - Detect any inconsistencies or anomalies.
    - Compare past logs with the expected monthly events and highlight missing or duplicate occurrences.

    Provide your analysis in a structured format.
    """

    # Invoke Claude
    return invoke_claude(training_prompt)

# Train Claude
training_summary = train_claude_on_logs()
print("\nðŸ”¹ Claude's Training Summary:\n", training_summary)

# ðŸš€ Step 2: Process New Logs & Track Duplicates
def process_new_logs(new_logs):
    """Process new logs, check for duplicates, and update storage."""
    global df_new_logs

    # Convert new logs into DataFrame
    new_logs_df = pd.DataFrame(new_logs, columns=["timestamp", "log_message"])

    # Identify duplicate logs (semantic search will refine this)
    duplicates = new_logs_df[new_logs_df["log_message"].isin(df_new_logs["log_message"])]

    # Append only unique logs
    df_new_logs = pd.concat([df_new_logs, new_logs_df]).drop_duplicates().reset_index(drop=True)
    save_new_logs(df_new_logs)

    return duplicates

# ðŸš€ Step 3: FAISS Embedding & Semantic Search
def get_embedding(text):
    """Generate an embedding using Amazon Titan."""
    response = bedrock.invoke_model(
        modelId=EMBED_MODEL_ID,
        contentType="application/json",
        accept="application/json",
        body=json.dumps({"inputText": text})
    )
    return np.array(json.loads(response["body"].read())["embedding"]).astype("float32")

def search_faiss(query_embedding, top_k=5):
    """Search for similar embeddings in FAISS."""
    query_embedding = query_embedding.reshape(1, -1)
    distances, indices = faiss_index.search(query_embedding, top_k)
    return df_logs.iloc[indices[0]]

def search_similar_logs(new_logs):
    """Embed new logs and find the most similar logs from this month."""
    similar_logs = []
    for log in new_logs:
        embedding = get_embedding(log)
        similar_logs.append(search_faiss(embedding))
    
    return pd.concat(similar_logs).drop_duplicates().reset_index(drop=True)

# ðŸš€ Step 4: Use Claude 3.5 for Anomaly Detection
def detect_anomalies_with_claude():
    """Analyze new logs and return structured results."""
    global df_new_logs

    # Search FAISS for similar logs
    similar_logs_df = search_similar_logs(df_new_logs["log_message"].tolist())

    # Convert logs into text format
    new_logs_str = "\n".join(df_new_logs["log_message"].tolist())
    similar_logs_str = "\n".join(similar_logs_df["log_message"].tolist())

    # Format expected events
    expected_events_str = "\n".join([
        f"- Process: {event['process_name']}, Data Set ID: {event['data_set_id']}, Event Type: {event['event_type_name']}"
        for event in EXPECTED_MONTHLY_EVENTS
    ])

    # Create structured prompt
    anomaly_detection_prompt = f"""
    You are analyzing system logs for anomalies.

    **1. Expected Monthly Events:**
    {expected_events_str}

    **2. Similar Past Logs (FAISS Search Results):**
    {similar_logs_str}

    **3. New Logs for This Month:**
    {new_logs_str}

    **Task:**
    - Compare new logs against similar past logs.
    - Detect duplicate events.
    - Identify any missing expected events.
    - Provide a structured JSON output.
    """

    # Invoke Claude
    response = invoke_claude(anomaly_detection_prompt)
    return json.loads(response)

# ðŸš€ Step 5: Generate Final Report
def generate_final_report(new_logs):
    """Run anomaly detection and compile results."""
    
    # Detect anomalies
    anomalies_output = detect_anomalies_with_claude()

    # Compile final structured report
    final_report = {
        "anomalies_found": anomalies_output["anomalies_found"],
        "duplicate_files_found": anomalies_output["duplicate_files_found"],
        "missing_events": anomalies_output["missing_events"]
    }

    print("\nðŸ”¹ Final Anomaly Detection Report:\n", json.dumps(final_report, indent=4))
    return final_report

# Example new logs received today
new_logs_today = [
    ("2025-03-11 10:15:00", "File upload: sales_data_2025_03.csv"),
    ("2025-03-11 10:20:00", "Processing error detected for sales_data_2025_03.csv"),
    ("2025-03-11 10:30:00", "Backup completed for sales_data_2025_03.csv")
]

# Process new logs and generate report
process_new_logs(new_logs_today)
final_report = generate_final_report([log[1] for log in new_logs_today])
