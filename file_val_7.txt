# -*- coding: utf-8 -*-
"""
DataFrame Validator against JSON DDL
- Reads DDL from a JSON file
- Validates presence, nullability, null percentage, types, lengths, categories
- Robust datetime parsing (ISO-8601, tz offsets); supports date-only comparison
- DATE bounds OFF by default (type-only); enable with enforce_date_bounds=True
- NUMERIC bounds OFF by default (type-only); enable with enforce_numeric_bounds=True
- When numeric bounds are enabled, supports margins (abs and/or percent), with per-column overrides
"""

import json
from typing import Dict, List, Tuple, Optional
import pandas as pd
import numpy as np

# -------------------- Global options --------------------
# Default comparison mode for DATE/DATETIME/TIMESTAMP columns:
#   "datetime" -> compare full timestamp (with time)
#   "date"     -> compare by calendar date only (ignoring time)
_DATE_COMPARE_MODE = "datetime"  # change to "date" for date-only comparisons globally

# -------------------- I/O --------------------

def load_ddl(path: str) -> dict:
    """Load the DDL JSON file and return it as a Python dict."""
    with open(path, "r") as f:
        data = f.read()
    return json.loads(data)

# -------------------- Helpers --------------------

def _as_ratio(v):
    """Return a float in [0,1] for inputs like 0.12, 12, '12%', or None."""
    if v is None:
        return None
    if isinstance(v, str):
        vs = v.strip()
        if vs.endswith("%"):
            try:
                return float(vs[:-1]) / 100.0
            except Exception:
                return None
        try:
            x = float(vs)
        except Exception:
            return None
    else:
        try:
            x = float(v)
        except Exception:
            return None
    return x / 100.0 if x > 1 else x

def _get_numeric_margins(meta: dict, global_abs: float, global_pct: float) -> Tuple[float, float]:
    """
    Per-column override of numeric bounds margins via metadata:
      - boundsMarginAbs
      - boundsMarginPct  (e.g., 0.05 for 5%)
    """
    abs_m = meta.get("boundsMarginAbs", global_abs)
    pct_m = meta.get("boundsMarginPct", global_pct)
    try:
        abs_m = float(abs_m)
    except Exception:
        abs_m = float(global_abs)
    try:
        pct_m = float(pct_m)
    except Exception:
        pct_m = float(global_pct)
    if abs_m < 0: abs_m = 0.0
    if pct_m < 0: pct_m = 0.0
    return abs_m, pct_m

def _apply_margin_to_bounds(
    minv: Optional[float],
    maxv: Optional[float],
    abs_m: float,
    pct_m: float
) -> Tuple[Optional[float], Optional[float], str]:
    """
    Compute adjusted [min,max] with margin:
      - Percent margin is based on (max - min) if both provided; else abs(bound)*pct
      - Effective margin = max(abs_m, percent_amount)
      - Returns (min_adj, max_adj, margin_info_str)
    """
    margin_info = []
    rng = None
    if minv is not None and maxv is not None:
        try:
            rng = float(maxv) - float(minv)
        except Exception:
            rng = None

    if rng is not None:
        pct_amt = abs(rng) * pct_m
        eff = max(abs_m, pct_amt)
        eff_min = eff_max = eff
    else:
        pct_amt_min = (abs(float(minv)) * pct_m) if minv is not None else 0.0
        pct_amt_max = (abs(float(maxv)) * pct_m) if maxv is not None else 0.0
        eff_min = max(abs_m, pct_amt_min) if minv is not None else None
        eff_max = max(abs_m, pct_amt_max) if maxv is not None else None

    min_adj = (float(minv) - float(eff_min)) if (minv is not None and eff_min is not None) else None
    max_adj = (float(maxv) + float(eff_max)) if (maxv is not None and eff_max is not None) else None

    if abs_m or pct_m:
        if rng is not None:
            margin_info.append(f"margin abs≤{abs_m:g}, pct≤{pct_m:.2%} (range-based)")
        else:
            parts = [f"abs≤{abs_m:g}"]
            if pct_m:
                parts.append(f"pct≤{pct_m:.2%}")
            margin_info.append("margin " + ", ".join(parts))
    margin_str = f" ({'; '.join(margin_info)})" if margin_info else ""
    return min_adj, max_adj, margin_str

# -------------------- Date/Time helpers --------------------

def parse_datetime_flex(series: pd.Series) -> pd.Series:
    """
    Robustly parse datetimes:
      - Accepts ISO8601 strings, fractional seconds, timezone offsets (e.g., ...-05:00)
      - Works with existing datetime-typed Series (naive or tz-aware)
    Returns UTC-aware pandas Timestamps (datetime64[ns, UTC]) or NaT.
    """
    s = series
    if np.issubdtype(s.dtype, np.datetime64):
        try:
            if getattr(s.dt, "tz", None) is not None:
                return s.dt.tz_convert("UTC")
            else:
                return s.dt.tz_localize("UTC")
        except Exception:
            pass
    return pd.to_datetime(s, errors="coerce", utc=True, infer_datetime_format=True)

def coerce_to_date(dt_utc: pd.Series) -> pd.Series:
    """Convert UTC-aware datetimes to date-only (drops time). Keeps NaT."""
    return dt_utc.dt.tz_convert("UTC").dt.normalize().dt.date

# -------------------- DDL helpers --------------------

def _ddl_columns_by_name(ddl: dict, normalize: bool = False) -> Dict[str, dict]:
    """Flatten DDL to {column_name: column_def}. Optionally strip names."""
    by_name = {}
    for block in ddl.get("ddl", []):
        for c in block.get("columns", []):
            name = c["name"]
            if normalize and isinstance(name, str):
                name = name.strip()
            by_name[name] = c
    return by_name

# -------------------- Core Validator --------------------

def validate_df_against_ddl(
    df: pd.DataFrame,
    ddl: dict,
    *,
    fail_on_unexpected: bool = False,
    normalize_column_names: bool = False,
    null_pct_tolerance: float = 0.02,     # ±2% allowed drift on metadata.nullPercentage
    enforce_date_bounds: bool = False,    # DATE bounds off by default
    enforce_numeric_bounds: bool = False, # NUMERIC bounds off by default (changed)
    numeric_bounds_margin_abs: float = 0.0,
    numeric_bounds_margin_pct: float = 0.0
):
    """
    Validate df against ddl.

    Returns:
      errors_df: DataFrame[row_index, column, issue, value]
      summary: {
        valid: bool,
        reasons: List[str],
        missing_required_columns: List[str],
        unexpected_columns: List[str],
        columns: Dict[col, {"errors": int, "nulls": int|None}],
        overlap_columns: List[str],
        ddl_only_columns: List[str],
        df_only_columns: List[str],
      }
    """
    # Optional normalization of column names
    if normalize_column_names:
        df = df.rename(columns=lambda c: c.strip() if isinstance(c, str) else c)

    by_name = _ddl_columns_by_name(ddl, normalize=normalize_column_names)
    ddl_cols = set(by_name.keys())
    df_cols = set(df.columns)

    required = {c for c, d in by_name.items()
                if not (d.get("type", {}) or {}).get("nullable", True)}
    missing_required = sorted(list(required - df_cols))
    unexpected = sorted(list(df_cols - ddl_cols))

    col_summary = {
        c: {"errors": 0, "nulls": (int(df[c].isna().sum()) if c in df_cols else None)}
        for c in ddl_cols
    }

    errors = []

    # Validate only overlapping columns
    for col in sorted(ddl_cols & df_cols):
        spec = by_name[col]
        typ = (spec.get("type", {}) or {}).get("datatype", "").upper()
        nullable = (spec.get("type", {}) or {}).get("nullable", True)
        declared_max_len = (spec.get("type", {}) or {}).get("maxLength", None)
        meta = spec.get("metadata", {}) or {}

        s = df[col]
        null_mask = s.isna() | (s.astype("string").str.strip() == "")

        # ---- Null percentage check (dataset-level) ----
        expected_null_pct = meta.get("nullPercentage", meta.get("null_percent"))
        exp_ratio = _as_ratio(expected_null_pct)
        if exp_ratio is not None:
            actual_ratio = float(null_mask.mean())
            if abs(actual_ratio - exp_ratio) > null_pct_tolerance:
                errors.append({
                    "row_index": -1,
                    "column": col,
                    "issue": (f"Null% mismatch: actual {actual_ratio:.2%}, "
                              f"expected {exp_ratio:.2%} ± {null_pct_tolerance:.2%}"),
                    "value": None
                })
                col_summary[col]["errors"] += 1

        # ---- Nullability per value ----
        if not nullable and null_mask.any():
            n_bad = int(null_mask.sum())
            col_summary[col]["errors"] += n_bad
            for i in s.index[null_mask]:
                errors.append({"row_index": int(i), "column": col,
                               "issue": "Null/empty but not nullable", "value": None})

        nn = s[~null_mask]

        # INT
        if typ in ("INT", "INTEGER"):
            coerced = pd.to_numeric(nn, errors="coerce", downcast="integer")
            bad_type = coerced.isna()
            if bad_type.any():
                n_bad = int(bad_type.sum()); col_summary[col]["errors"] += n_bad
                for i, v in nn[bad_type].items():
                    errors.append({"row_index": int(i), "column": col, "issue": "Expected INT", "value": v})
            good_vals = coerced[~bad_type]

            # Per-column override for numeric bounds
            col_enforce_numeric = meta.get("enforceNumericBounds")
            if col_enforce_numeric is None:
                col_enforce_numeric = enforce_numeric_bounds

            if col_enforce_numeric:
                minv = meta.get("minValue")
                maxv = meta.get("maxValue")
                if minv is not None or maxv is not None:
                    abs_m, pct_m = _get_numeric_margins(meta, numeric_bounds_margin_abs, numeric_bounds_margin_pct)
                    min_adj, max_adj, margin_str = _apply_margin_to_bounds(minv, maxv, abs_m, pct_m)

                    if min_adj is not None:
                        mask = good_vals < float(min_adj)
                        if mask.any():
                            n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                            for i, v in good_vals[mask].items():
                                errors.append({"row_index": int(i), "column": col,
                                               "issue": f"< minValue {minv}{margin_str}", "value": int(v)})
                    if max_adj is not None:
                        mask = good_vals > float(max_adj)
                        if mask.any():
                            n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                            for i, v in good_vals[mask].items():
                                errors.append({"row_index": int(i), "column": col,
                                               "issue": f"> maxValue {maxv}{margin_str}", "value": int(v)})

        # FLOAT
        elif typ in ("FLOAT", "DOUBLE", "DECIMAL", "NUMERIC"):
            coerced = pd.to_numeric(nn, errors="coerce")
            bad_type = coerced.isna()
            if bad_type.any():
                n_bad = int(bad_type.sum()); col_summary[col]["errors"] += n_bad
                for i, v in nn[bad_type].items():
                    errors.append({"row_index": int(i), "column": col, "issue": "Expected FLOAT", "value": v})
            good_vals = coerced[~bad_type]

            col_enforce_numeric = meta.get("enforceNumericBounds")
            if col_enforce_numeric is None:
                col_enforce_numeric = enforce_numeric_bounds

            if col_enforce_numeric:
                minv = meta.get("minValue")
                maxv = meta.get("maxValue")
                if minv is not None or maxv is not None:
                    abs_m, pct_m = _get_numeric_margins(meta, numeric_bounds_margin_abs, numeric_bounds_margin_pct)
                    minf = float(minv) if minv is not None else None
                    maxf = float(maxv) if maxv is not None else None
                    min_adj, max_adj, margin_str = _apply_margin_to_bounds(minf, maxf, abs_m, pct_m)

                    if min_adj is not None:
                        mask = good_vals < float(min_adj)
                        if mask.any():
                            n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                            for i, v in good_vals[mask].items():
                                errors.append({"row_index": int(i), "column": col,
                                               "issue": f"< minValue {minv}{margin_str}", "value": float(v)})
                    if max_adj is not None:
                        mask = good_vals > float(max_adj)
                        if mask.any():
                            n_bad = int(mask.sum()); col_summary[col]["errors"] += n_bad
                            for i, v in good_vals[mask].items():
                                errors.append({"row_index": int(i), "column": col,
                                               "issue": f"> maxValue {maxv}{margin_str}", "value": float(v)})

        # DATE / DATETIME / TIMESTAMP
        elif typ in ("DATE", "DATETIME", "TIMESTAMP"):
            parsed_utc = parse_datetime_flex(nn)
            bad_type = parsed_utc.isna()
            if bad_type.any():
                n_bad = int(bad_type.sum()); col_summary[col]["errors"] += n_bad
                for i, v in nn[bad_type].items():
                    errors.append({
                        "row_index": int(i), "column": col,
                        "issue": "Unparseable datetime (ISO 8601 expected)",
                        "value": v
                    })

            good_vals = parsed_utc[~bad_type]

            # Choose comparison mode (only relevant if bounds are enforced)
            mode = meta.get("dateCompareMode") or _DATE_COMPARE_MODE
            if mode not in ("datetime", "date"):
                mode = "datetime"

            col_enforce_bounds = meta.get("enforceDateBounds")
            if col_enforce_bounds is None:
                col_enforce_bounds = enforce_date_bounds

            if col_enforce_bounds:
                # Parse min/max strictly with pandas (utc-aware)
                minv, maxv = meta.get("minValue"), meta.get("maxValue")
                min_parsed = pd.to_datetime(pd.Series([minv]), errors="coerce", utc=True).iloc[0] if minv is not None else pd.NaT
                max_parsed = pd.to_datetime(pd.Series([maxv]), errors="coerce", utc=True).iloc[0] if maxv is not None else pd.NaT

                if mode == "date":
                    good_cmp = coerce_to_date(good_vals)
                    min_cmp = (min_parsed.tz_convert("UTC").normalize().date() if pd.notna(min_parsed) else None)
                    max_cmp = (max_parsed.tz_convert("UTC").normalize().date() if pd.notna(max_parsed) else None)
                else:
                    good_cmp = good_vals
                    min_cmp = min_parsed if pd.notna(min_parsed) else None
                    max_cmp = max_parsed if pd.notna(max_parsed) else None

                if min_cmp is not None:
                    oob = good_cmp < min_cmp
                    if oob.any():
                        n_bad = int(oob.sum()); col_summary[col]["errors"] += n_bad
                        for i, v in good_vals[oob].items():
                            errors.append({"row_index": int(i), "column": col,
                                           "issue": f"< minValue {minv}", "value": v.isoformat()})
                if max_cmp is not None:
                    oob = good_cmp > max_cmp
                    if oob.any():
                        n_bad = int(oob.sum()); col_summary[col]["errors"] += n_bad
                        for i, v in good_vals[oob].items():
                            errors.append({"row_index": int(i), "column": col,
                                           "issue": f"> maxValue {maxv}", "value": v.isoformat()})

        # VARCHAR
        elif typ in ("VARCHAR", "CHAR"):
            s_str = nn.astype("string")
            cat_ratios = meta.get("categoryRatios")
            if isinstance(cat_ratios, dict) and len(cat_ratios) > 0:
                allowed = set(cat_ratios.keys())
                bad = ~s_str.isin(allowed)
                if bad.any():
                    n_bad = int(bad.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[bad].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": "Value not in allowed categories", "value": v})
            max_len = declared_max_len if declared_max_len is not None else meta.get("maxCharLength")
            if isinstance(max_len, int):
                too_long = s_str.str.len() > max_len
                if too_long.any():
                    n_bad = int(too_long.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[too_long].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": f"Length > {max_len}", "value": v})

        # TEXT
        elif typ == "TEXT":
            s_str = nn.astype("string")
            min_len, max_len = meta.get("minCharLength"), meta.get("maxCharLength")
            if isinstance(min_len, int):
                too_short = s_str.str.len() < min_len
                if too_short.any():
                    n_bad = int(too_short.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[too_short].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": f"Length < {min_len}", "value": v})
            if isinstance(max_len, int):
                too_long = s_str.str.len() > max_len
                if too_long.any():
                    n_bad = int(too_long.sum()); col_summary[col]["errors"] += n_bad
                    for i, v in s_str[too_long].items():
                        errors.append({"row_index": int(i), "column": col,
                                       "issue": f"Length > {max_len}", "value": v})

        else:
            # Unknown datatype — informational note once per column (only if data present)
            if len(nn) > 0:
                errors.append({"row_index": -1, "column": col,
                               "issue": f"Unrecognized datatype '{typ}' (skipped strict checks)",
                               "value": None})

    # Assemble outputs
    errors_df = pd.DataFrame(errors, columns=["row_index", "column", "issue", "value"]) \
                   .sort_values(["column", "row_index"], ignore_index=True)

    # Build reasons & final validity
    reasons = []
    if missing_required:
        reasons.append(f"Missing required columns: {missing_required}")
    if not errors_df.empty:
        reasons.append(f"Row-level errors: {len(errors_df)}")
    if fail_on_unexpected and len(unexpected) > 0:
        reasons.append(f"Unexpected columns: {unexpected}")

    summary = {
        "valid": (len(missing_required) == 0)
                 and errors_df.empty
                 and (not fail_on_unexpected or len(unexpected) == 0),
        "reasons": reasons,
        "missing_required_columns": missing_required,
        "unexpected_columns": unexpected,
        "columns": col_summary,
        "overlap_columns": sorted(list(ddl_cols & df_cols)),
        "ddl_only_columns": sorted(list(ddl_cols - df_cols)),
        "df_only_columns": sorted(list(df_cols - ddl_cols)),
    }

    return errors_df, summary
